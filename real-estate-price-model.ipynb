{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c9c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc55393",
   "metadata": {},
   "source": [
    "# Load initial dataset\n",
    "Explore bostonhousing dataset and generate intermediate processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc0e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    file_name_without_extension = \"bostonhousing-693729dedb019653836667\"\n",
    "    original_file_path = Path(\"./datas\") / f\"{file_name_without_extension}.csv\"\n",
    "    clean_dataset_name = Path(\"./datas\") / \"cleaned_dataset.csv\"\n",
    "\n",
    "    try:\n",
    "        full_pd_dataset = pd.read_csv(original_file_path)\n",
    "        logger.info(f\"Successfully loaded data from {original_file_path}\")\n",
    "        logger.info(f\"Data shape: {full_pd_dataset.shape}\")\n",
    "\n",
    "        return full_pd_dataset\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"{original_file_path.absolute()} was not found\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Something went wrong loading data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab20dd",
   "metadata": {},
   "source": [
    "## iqr outlier detection\n",
    "IQR (Inter Quartile Range) : Determine \"outlier\" all values out of the interval `[Q1 - f * IQR, Q3 + f * IQR]` where f is a factor (std 1.5) and IQR is Q3 - Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be274f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_detection(df, column, factor=1.5):\n",
    "    \"\"\"\n",
    "        IQR outlier detection in a specific column\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataframe\n",
    "            column (str): column name to analyze\n",
    "            factor (float): multiplicant factor for IQR (standard : 1.5)\n",
    "        Returns:\n",
    "            pd.Series : boolean mask (True = outlier, False = non outlier)\n",
    "    \"\"\"\n",
    "\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - factor * IQR\n",
    "    upper_bound = Q3 + factor * IQR\n",
    "\n",
    "    outlier_mask = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "\n",
    "    return outlier_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa62a29",
   "metadata": {},
   "source": [
    "## z-score outlier detection\n",
    "Z-score count all standard deviation from one observation to mean. Standard threshold +/- 3 standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3454e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_detection(df, column, threshold=3):\n",
    "    \"\"\"\n",
    "    Z-score outlier detection for one column\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        column (str): Column to analyze\n",
    "        threshold (float): Z-Score thersold (standard = 3.0).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: boolean mask (True = outlier, False = non-outlier).\n",
    "    \"\"\"\n",
    "    # Z-score\n",
    "    z_scores = np.abs((df[column] - df[column].mean()) / df[column].std())\n",
    "    \n",
    "    # Mask: True if Z-score > threshold (outlier)\n",
    "    outlier_mask = z_scores > threshold\n",
    "    return outlier_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cacc5ad",
   "metadata": {},
   "source": [
    "## Isolation forest outlier detection\n",
    "Isolation Forest is a non supervised learning algorithm isolating abnormal observations. Require scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d2c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def isolation_forest_detection(df, column, contamination='auto', random_state=42):\n",
    "    \"\"\"\n",
    "    Outlier detection using Isolation Forest algorithm.\n",
    "    \n",
    "    NOTE: This algorithm is multi-dimensional. Here it is applied \n",
    "          to unique column to keep coherence with other outlier detection methods.\n",
    "          For better performance it needs multiple relevant columns\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        column (str): Column to test\n",
    "        contamination (str/float): Expected outliers number in dataframe\n",
    "        random_state (int): Reproductibility\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: boolean mask (True = outlier, False = non-outlier).\n",
    "    \"\"\"\n",
    "    # Algorithm require an input as 2D (N, 1)\n",
    "    X = df[[column]].values\n",
    "    \n",
    "    model = IsolationForest(\n",
    "        contamination=contamination, \n",
    "        random_state=random_state,\n",
    "        n_jobs=-1 # Use all cores\n",
    "    )\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(X)\n",
    "    \n",
    "    # predict method returns: 1 for inliers, -1 for outliers\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    # Mask: True if prediction is -1 (outlier)\n",
    "    outlier_mask = pd.Series(predictions == -1, index=df.index)\n",
    "    return outlier_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8afe405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_detector(df, column, algorithm, **kwargs):\n",
    "    \"\"\"\n",
    "    Apply one of the outlier detection function and returns cleaned dataset\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        column (str): Column name to analyze.\n",
    "        algorithm (function): Detection function (e.g., iqr_detection).\n",
    "        **kwargs: Specifics args to give to function (e.g., factor, threshold).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe without outliers identified\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        logger.error(\"No dataframe. Detection aborted\")\n",
    "        return None\n",
    "    \n",
    "    algo_name = algorithm.__name__.replace('_detection', '').upper()\n",
    "    logger.info(f\"Running detection using : {algo_name}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Get outliers mask\n",
    "        outlier_mask = algorithm(df, column, **kwargs)\n",
    "        \n",
    "        # 2. Count outliers\n",
    "        num_outliers = outlier_mask.sum()\n",
    "        total_rows = len(df)\n",
    "        logger.info(f\"{num_outliers} outliers was detected in '{column}' ({num_outliers/total_rows:.2%})\")\n",
    "        \n",
    "        # 3. Build cleaned dataframe (Mask False = non-outlier)\n",
    "        cleaned_df = df[~outlier_mask].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "        \n",
    "        logger.success(f\"Outliers cleaning done. DataFrame reduce to {total_rows} à {len(cleaned_df)} rows.\")\n",
    "        \n",
    "        return cleaned_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error using {algo_name}: {e}\")\n",
    "        return df # Return original dataframe avoiding to break the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05abec66",
   "metadata": {},
   "source": [
    "## Normalization step\n",
    "From a cleaned dataframe, apply a normalisation algorithm for all columns using MinMax scaling.\n",
    "\n",
    "Xnorm = X - Xmin / Xmax - Xmin\n",
    "\n",
    "Using this method we expect that all characteristics are in the same range ([0, 1]), distributions are kept, no problem with outliers already detected.\n",
    "\n",
    "On Boston datas MinMax is better than MaxAbs cause : datas are already positives. MaxAbs is better on sparse data or datas are 0 centered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a691cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def minmax_normalize(df, target_column='medv', binary_columns=['chas']):\n",
    "    \"\"\"\n",
    "    Normalize numeric columns using MinMax Scaling (0 à 1).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        target_column (str): Target column (not normalized).\n",
    "        binary_columns (list): Binary columns (not normalized).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: normalized dataframe.\n",
    "        MinMaxScaler: Adjusted Scaler, usefull to future denormalization.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        logger.error(\"No dataframe. Normalization aborted.\")\n",
    "        return None, None\n",
    "\n",
    "    # 1. Identify all columns to normalize (expect target and binaries)\n",
    "    columns_to_scale = [\n",
    "        col for col in df.columns \n",
    "        if col not in [target_column] and col not in binary_columns\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"Columns to normalize ({len(columns_to_scale)}): {columns_to_scale}\")\n",
    "\n",
    "    # 2. Créer et ajuster le scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Adujst scaler only for training columns\n",
    "    # Use .values.reshape(-1, 1) to make sur that Pandas will correctly handle transformation\n",
    "    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "    \n",
    "    logger.success(\"MinMax normalization successfully applyed on features.\")\n",
    "    \n",
    "    return df, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b8573e",
   "metadata": {},
   "source": [
    "## Standardization using z-score\n",
    "\n",
    "Observing regression model, this function should be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c8822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "\n",
    "\n",
    "def zscore_standardize(df, target_column='medv', binary_columns=['chas']):\n",
    "    \"\"\"\n",
    "    Z-Score standardization for all numerical columns\n",
    "    (Mean=0, Standard variation=1).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        target_column (str): Target column (non standard).\n",
    "        binary_columns (list):Binaries columns (non standard).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Standardized dataframe.\n",
    "        StandardScaler: Adujsted scaler, if we need to destandardized.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        logger.error(\"No dataframe. Standardization aborted.\")\n",
    "        return None, None\n",
    "\n",
    "    # 1. Identify columns to process\n",
    "    columns_to_scale = [\n",
    "        col for col in df.columns \n",
    "        if col not in [target_column] and col not in binary_columns\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"Columns to process ({len(columns_to_scale)}): {columns_to_scale}\")\n",
    "\n",
    "    # 2. Create and adjust scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Ajuster le scaler uniquement sur les colonnes d'entraînement\n",
    "    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "    \n",
    "    logger.success(\"Z-Score successfully applied on features\")\n",
    "    \n",
    "    return df, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ada040",
   "metadata": {},
   "source": [
    "## NaN detection and imputation using one of the method : median, mean or constant\n",
    "\n",
    "NaN can issue some deviance. So, we impute data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c38ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "def impute_missing_values(\n",
    "    df: pd.DataFrame, \n",
    "    strategy: Literal['median', 'mean', 'constant'], \n",
    "    constant_value=None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detect and impute missing values (NaN)\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        strategy (Literal['median', 'mean', 'constant']): Imputation method to use\n",
    "        constant_value (float, optional): Constant value to input if constant strategy\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe after imputation.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        logger.error(\"No dataframe. Imputation aborted.\")\n",
    "        return None\n",
    "\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Identify numerical columns (statistical imputation)\n",
    "    # Exclude 'object' types (strings)\n",
    "    numeric_cols = df_imputed.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    logger.info(f\"Start imputation using : {strategy.upper()} strategy\")\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        # 1. NaN detection\n",
    "        nan_count = df_imputed[col].isnull().sum()\n",
    "        \n",
    "        if nan_count == 0:\n",
    "            continue\n",
    "        \n",
    "        # 2. Imputation value determination according strategy\n",
    "        impute_val = None\n",
    "        \n",
    "        if strategy == 'median':\n",
    "            impute_val = df_imputed[col].median()\n",
    "        elif strategy == 'mean':\n",
    "            impute_val = df_imputed[col].mean()\n",
    "        elif strategy == 'constant':\n",
    "            impute_val = constant_value\n",
    "            if impute_val is None:\n",
    "                logger.warning(f\"'constant' strategy used for '{col}' but constant_value is None. Ignored.\")\n",
    "                continue\n",
    "        \n",
    "        # 3. Imputation\n",
    "        df_imputed[col].fillna(impute_val, inplace=True)\n",
    "        logger.debug(f\"Column '{col}': {nan_count} NaN values imputed with {impute_val:.4f} ({strategy}).\")\n",
    "        \n",
    "    # Final check\n",
    "    if df_imputed.isnull().sum().sum() == 0:\n",
    "        logger.success(\"Imputation done : No NaN detected.\")\n",
    "    else:\n",
    "        logger.warning(\"Warning : Some NaN detected (probably non numerical columns).\")\n",
    "        \n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70e2369",
   "metadata": {},
   "source": [
    "## Artifacts : graphs\n",
    "\n",
    "- residual,\n",
    "- predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b87593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "\n",
    "def save_and_log_regression_plots(model, X_train, y_train, run_id):\n",
    "    \"\"\"Generate and store residual and prediction graphs.\"\"\"\n",
    "    \n",
    "    # 1. Artefact folder\n",
    "    artifact_path = f\"mlruns/{run_id}/artifacts/plots\"\n",
    "    os.makedirs(artifact_path, exist_ok=True)\n",
    "    \n",
    "    # Predict on training dataset for plots\n",
    "    y_pred = model.predict(X_train)\n",
    "    \n",
    "    # --- Plot #1 : residual ---\n",
    "    residues = y_train - y_pred\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_pred, residues, alpha=0.5)\n",
    "    plt.hlines(y=0, xmin=y_pred.min(), xmax=y_pred.max(), color='red', linestyle='--')\n",
    "    plt.title('Residual graph')\n",
    "    plt.xlabel('Predicted values')\n",
    "    plt.ylabel('Residual (Real - Predicted)')\n",
    "    resid_plot_path = os.path.join(artifact_path, \"residuals_plot.png\")\n",
    "    plt.savefig(resid_plot_path)\n",
    "    plt.close() # Free memory\n",
    "    mlflow.log_artifact(resid_plot_path, \"plots\")\n",
    "    logger.debug(\"Residual graph stored\")\n",
    "\n",
    "    # --- Plot #2 : Predicted vs Real ---\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(y_train, y_pred, alpha=0.5)\n",
    "    plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')\n",
    "    plt.title('Predicted vs. Real values')\n",
    "    plt.xlabel('Real values')\n",
    "    plt.ylabel('Predicted values')\n",
    "    pred_plot_path = os.path.join(artifact_path, \"predictions_vs_actual.png\")\n",
    "    plt.savefig(pred_plot_path)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(pred_plot_path, \"plots\")\n",
    "    logger.debug(\"Predicted plot stored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9bf8ef",
   "metadata": {},
   "source": [
    "## Training and logging model using MLFlow\n",
    "Training will be based on :\n",
    "\n",
    "- CV (Cross validation),\n",
    "- RMSE (Root Mean Squared Error) as scoring metric\n",
    "- R2 scoring metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "\n",
    "def train_and_log_model(\n",
    "    model, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    model_name, \n",
    "    n_splits=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Train model, Cross Validation performance measurement, store all with MLFlow\n",
    "    \"\"\"\n",
    "    logger.info(f\"MLFlow training experience for: {model_name}\")\n",
    "    \n",
    "    # Metrics settings\n",
    "    cv_scoring_metrics = {\n",
    "        'neg_mean_squared_error': 'neg_mean_squared_error',\n",
    "        'r2': 'r2'\n",
    "    }\n",
    "\n",
    "    # Start MLFlow run\n",
    "    with mlflow.start_run(run_name=model_name) as run:\n",
    "        \n",
    "        run_id = run.info.run_id\n",
    "\n",
    "        # 1. Cross Validation (CV)\n",
    "        # Using KFlold (5 splits good default value)\n",
    "        cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Using RMSE (Root Mean Squared Error) as scoring metric\n",
    "        # cross_val_score use default negative scoring (neg_mean_squared_error)\n",
    "        cv_results = cross_validate(\n",
    "            model, \n",
    "            X_train, \n",
    "            y_train, \n",
    "            scoring=cv_scoring_metrics, \n",
    "            cv=cv, \n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # RMSE : Convert to positive RMSE\n",
    "        rmse_scores = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n",
    "        mlflow.log_metric(\"cv_mean_rmse\", rmse_scores.mean())\n",
    "        mlflow.log_metric(\"cv_std_rmse\", rmse_scores.std())\n",
    "        logger.success(f\"CV Mean RMSE for {model_name}: {rmse_scores.mean():.3f}\")\n",
    "\n",
    "        # R2\n",
    "        r2_scores = cv_results['test_r2']\n",
    "        mlflow.log_metric(\"cv_mean_r2\", r2_scores.mean())\n",
    "        mlflow.log_metric(\"cv_std_r2\", r2_scores.std())\n",
    "        logger.info(f\"CV mean R2 for {model_name}: {r2_scores.mean():.3f}\")\n",
    "        \n",
    "        # 2. Final training (on the whole training dataset)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Plots and store results\n",
    "        save_and_log_regression_plots(model, X_train, y_train, run_id)\n",
    "        \n",
    "        # 3. MLFlow logging\n",
    "        \n",
    "        # Hyperparameters storage\n",
    "        mlflow.log_params(model.get_params())\n",
    "        \n",
    "        # CV metrics storage\n",
    "        mlflow.log_params(model.get_params())\n",
    "        \n",
    "        logger.success(f\"Mean CV RMSE for {model_name}: {rmse_scores.mean():.3f}\")\n",
    "        \n",
    "        # Save final trained model\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c38c94",
   "metadata": {},
   "source": [
    "## main : Handle all processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = load_data()\n",
    "\n",
    "if df is not None:\n",
    "    logger.info(\"Datas was sucessfully loaded and stored in 'df'\")\n",
    "else:\n",
    "    logger.debug(\"No data was loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a15a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just print a few lines...\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e3ddf",
   "metadata": {},
   "source": [
    "## Outlier detection\n",
    "Process outlier detection using one of the algorithm : iqr, z-score or isolation forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ce9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply outliers detection\n",
    "\n",
    "TARGET_COLUMN = \"medv\"\n",
    "ALGORITHM = \"iqr_detection\" # Can be : isolation_forest_detection | zscore_detection\n",
    "\n",
    "df_iqr_cleaned = outlier_detector(\n",
    "    df,\n",
    "    column=TARGET_COLUMN,\n",
    "    algorithm=iqr_detection,\n",
    "    factor=1.5\n",
    ")\n",
    "df_iqr_cleaned.head()\n",
    "\n",
    "df_zscore_cleaned = outlier_detector(\n",
    "    df,\n",
    "    column=TARGET_COLUMN,\n",
    "    algorithm=zscore_detection,\n",
    "    threshold=3.0\n",
    ")\n",
    "df_zscore_cleaned.head()\n",
    "\n",
    "df_if_cleaned = outlier_detector(\n",
    "    df=df,\n",
    "    column=TARGET_COLUMN,\n",
    "    algorithm=isolation_forest_detection,\n",
    "    contamination=0.05 # Considering about 5% of outliers\n",
    ")\n",
    "df_if_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c9f1bd",
   "metadata": {},
   "source": [
    "## Apply normalization using MinMax algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8499997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dataframe reduced by iqr in the pipeline\n",
    "df_normalized, scaler = minmax_normalize(\n",
    "    df_iqr_cleaned,\n",
    "    target_column=TARGET_COLUMN,\n",
    "    binary_columns=[\"chas\"]\n",
    ")\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d1495",
   "metadata": {},
   "source": [
    "## Appply imputation using \"mean\" strategy\n",
    "Mean better applies to normalized, near \"normal\" and outliers cleaned datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb24f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed_mean = impute_missing_values(\n",
    "    df_normalized,\n",
    "    strategy=\"mean\"\n",
    ")\n",
    "df_imputed_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b219a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare trained and predicted datas from df_imputed_means (last clean dataset)\n",
    "X = df_imputed_mean.drop(columns=[TARGET_COLUMN])\n",
    "y = df_imputed_mean[TARGET_COLUMN]\n",
    "\n",
    "# Split dataset in training and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bfb56e",
   "metadata": {},
   "source": [
    "## Concrete training using multiple strategies\n",
    "\n",
    "1. LinearRegression : good start to establish base performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f804327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "trained_lr = train_and_log_model(\n",
    "    model=lr_model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    model_name=\"Linear_Regression_Baseline\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87617968",
   "metadata": {},
   "source": [
    "## Concrete training using multiple strategies\n",
    "\n",
    "2. Random Forest : strong through scaling issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "trained_rf = train_and_log_model(\n",
    "    model=rf_model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    model_name=\"Random_Forest_Default\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d336f7",
   "metadata": {},
   "source": [
    "## Concrete training using multiple strategies\n",
    "\n",
    "3. lgbm_model : Accurate algorithm especially for tabular datas, often fastest than XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2305c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgbm_model = lgb.LGBMRegressor(random_state=42, n_jobs=-1)\n",
    "trained_lgbm = train_and_log_model(\n",
    "    model=lgbm_model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    model_name=\"LightGBM_Default\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff08020",
   "metadata": {},
   "source": [
    "## Concrete training using multiple strategies\n",
    "\n",
    "4. K-Nearest Neighbors : Distance models can be accurate on normalized and standardized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn_model = KNeighborsRegressor(n_neighbors=5) # 5 neighbors is a good default value\n",
    "trained_knn = train_and_log_model(\n",
    "    model=knn_model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    model_name=\"KNN_Distance_Based\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d7685",
   "metadata": {},
   "source": [
    "## Final step : save the best model\n",
    "\n",
    "To use it in the future API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872acde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler # ou StandardScaler, selon votre choix\n",
    "\n",
    "# Best model according m2 : lgbm\n",
    "BEST_MODEL = trained_lgbm\n",
    "# Assurez-vous d'avoir le scaler utilisé lors de la préparation des données\n",
    "SCALER = scaler # L'objet retourné par minmax_normalize ou zscore_standardize\n",
    "\n",
    "# Model directory\n",
    "MODEL_DIR = \"model_artefacts\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# 1. Trained model saving\n",
    "joblib.dump(BEST_MODEL, os.path.join(MODEL_DIR, \"best_regressor.joblib\"))\n",
    "\n",
    "# 2. Adjusted scaler saving\n",
    "joblib.dump(SCALER, os.path.join(MODEL_DIR, \"feature_scaler.joblib\"))\n",
    "\n",
    "logger.success(f\"Model and Scaler successfuly saved in : {MODEL_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
